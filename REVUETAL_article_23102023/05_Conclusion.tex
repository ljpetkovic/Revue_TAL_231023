Dans ce travail, nous avons mené des expériences sur la correction automatique des contaminations de la ROC, avec l'objectif de mesurer l'impact de ces corrections sur la REN spatiales. Nous avons établi (i) une typologie pour l'évaluation plus fine de l'impact des contaminations de la ROC sur les sorties de REN pour pallier le problème de l'évaluation automatique stricte, selon laquelle des cas particuliers d'EN contaminées étaient considérés comme des FP alors qu'il s'agit de VP, et, (ii) une typologie des contaminations de correction automatique de la ROC afin de rendre compte des fluctuations au niveau de la performance du correcteur. Notre étude s'appuie sur les corpus littéraires ELTeC (ouvrages en anglais, français et portugais), ainsi que sur celui de la TGB (ouvrages en français), dont les versions de ROC que nous avons générées ont été corrigées à l'aide de deux modèles de l'outil JamSpell : l'un fourni par défaut pour l'anglais et le français et l'autre entraîné sur le corpus ELTeC selon la langue adéquate. Les résultats ont montré que, contre-intuitivement, la correction automatique introduit des biais, notamment des sur-corrections, dans les données textuelles et que le gain apporté par les corrections justes n'était pas considérable.  Par ailleurs, les résultats du correcteur automatique sont plus signifiants dans le cas des textes plus bruités. Pour preuve, nous observons une réduction plus importante du nombre des hapax dans les sorties de REN sur l'outil de ROC Kraken qui est moins performant que Tesseract. Enfin, nous concluons que l'évaluation automatique de l'impact de la ROC sur la REN n'est pas une tâche triviale, et que sa complexité s'étend sur l'évaluation de la REN sur des textes de ROC corrigés ; il apparaît nécessaire de croiser les résultats de multiples méthodes d'évaluation pour affiner notre propos, c'est pourquoi nous employons différentes métriques ainsi que l'outil \textsc{NERVAL}. Dans la suite de notre travail, nous nous appuierons sur l'utilisation d'un autre outil de correction qui utilise des réseaux de neurones, qui serait susceptible de corriger automatiquement les contaminations de la ROC de manière plus probante.

% Concernant l'évaluation du modèle JamSpell--ELTeC pour les ouvrages en anglais et en français transcrits avec Tesseract, nous avons constaté un gain de performance léger mais quasi systématique. 
% À contrario, le modèle pré-entraîné semble plus souvent apporter des sur-corrections.

% \begin{itemize}
% \item \sout{Nous avons établi (i) une typologie des contaminations d’OCR et (ii) une typologie des erreurs de la correction automatique.}
%     \item \sout{Si la correction automatique des ocr fonctionne il y aura moins d'hapax dans la sortie REN.}
%     \item \sout{Lorsqu'on compare une évaluation automatique stricte de la REN sur ocr bruiyé avec une évaluation manuelle on observe qu'en fait on consrve les VP même sur données bruitées, mais qu'on a des variantes des EN qui dans l'évaluation automatique ne sont pas aligné avec leur forme conforme orthographiquement? C'est pourquoi nous avons produis la typologie 1.}
%     \item \sout{L'outil de correction produit des sur corrections}
%     \item \sout{Plus un texte est bruité plus l'ouitl de correction peut être pertinent}
%     \item \sout{l'évaluation automatique de l'impact de l'ocr sur la NER n'est pas une tâche triviale et donc la complexité de l'évaluation de la correction de l'ocr pour la ner hérite de cette complexité c'est pourquoi il est intéressant de multiplié la manière d'évalué et et que nous avons donc proposé d'utilsé des métriques différente et un outil NERVAL.}
% \end{itemize}
