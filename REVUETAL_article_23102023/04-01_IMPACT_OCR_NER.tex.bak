\section{Problématiques d'évaluation de l'impact de la ROC sur la REN} 
\label{sec:OCR-IMPACT-NER}
%Cette section décrit les outils de ROC et de REN, dont nous proposons des évaluations manuelle et automatique. Nous exposons des problématiques liées aux types d'évaluations stricte ou souple utilisés, ainsi qu'une typologie des contaminations de ROC pour une interprétation fine de l'évaluation stricte.

\subsection{Outils de ROC et REN utilisés dans le cadre de cette étude}
\label{subsec:outils_OCR-IMPACT-NER}

\subsubsection{Les outils de ROC}
Les transcriptions issues de la ROC ont été produites avec deux systèmes disponibles gratuitement : Kraken \cite{kiessling2019kraken} et Tesseract \cite{smith2007overview}. Bien qu'il existe un modèle pour le français du XVII\ieme{}siècle \cite{gabay:hal-02577236}\footnote{\url{https://github.com/Heresta/OCR17plus/tree/main/Model}}, ainsi que le modèle Gallicorpora \cite{pinche_2022_7410529}, nous ne les avons pas jugés adaptés à notre corpus, et donc nous utilisons le modèle de base de Kraken, version 3.0. Ce modèle permet d'opérer la segmentation\footnote{Le modèle de segmentation est constitué d'un réseau d'étiquetage par classification des phrases (angl. \textit{seed-labeling network}).
%qui est un \textit{U-net} (\og{}réseau entièrement convolutionnel\fg{}) modifié \cite{ronneberger2015} sur la base d'un réseau neuronal résiduel (ResNet) à 34 couches \cite{he2016}, pré-entraîné sur ImageNet.
} et la transcription\footnote{Le modèle de transcription fonctionne comme un classifieur de séquences sans segmentation qui utilise un réseau neuronal artificiel pour mapper une image d'une ligne de texte (séquence d'entrée), en une séquence de caractères (séquence de sortie). 
%Le réseau de neurones convolutif et récurrent a été entraîné avec la fonction de perte CTC (angl. \textit{Connexionist Temporal Classification} \cite{graves2006connectionist}).
}.
%ainsi qu'un modèle HTR\footnote{Abbr. angl. \textit{Handwritten Text Recognition} ou la reconnaissance d'écriture manuscrite, basée sur l'extraction du texte à partir des blocs textuels (lignes), contrairement à la méthode de ROC qui traite les caractères individuelles \cite{gabayscicos}.}
 Concernant Tesseract, nous avons utilisé le modèle LSTM \texttt{tessdata\_best}, sur la version 4.1.2 du système, entraîné sur des données Google. Tesseract propose une analyse de la mise en page intégrée à travers la segmentation des cadres (angl. \textit{box segmentation}), ce qui rend le traitement des mises en page complexes plus difficile \cite{reul2017case}. Le modèle de base de Tesseract est un modèle conçu pour l'anglais, et il existe un modèle français et un modèle portugais. Quatre modèles de langue neuronaux pour la ROC ont été utilisés dans le cadre de ces expériences : Kraken de base, Tesseract anglais, français et portugais contemporain.
%\footnote{Le modèle par défaut de Tesseract est l'anglais. Nous précisons la langue Tess. eng, Tess. fr et Tess. suivant les bonnes pratiques de \#RègledeBender \cite{bender-friedman-2018-data}.} et (iii) 

\subsubsection{Les outils de REN}
Pour effectuer la tâche de REN nous avons utilisé la chaîne de traitements de la boîte à outils pour le TAL \texttt{spaCy} dans sa version 3.5.1. Le choix de cet outil est déterminé par la multitude des modèles, leurs performances et leur facilité de la prise en main \cite{DBLP:conf/gis/Koudoro-Parfait21}. %  et \texttt{stanza}\footnote{1.5.0}. 
Le système \texttt{spaCy} contient une stratégie d'intégration de mots utilisant des fonctionnalités de sous-mots et les plongements ``Bloom'' (angl. \textit{Bloom embeddings})\footnote{Il s'agit de la structure de données probabiliste qui permet de réduire la dimension des vecteurs (\textit{cf.} \url{https://explosion.ai/blog/bloom-embeddings}).}, ainsi qu'un réseau neuronal convolutif avec des connexions résiduelles, ce qui peut expliquer sa robustesse lors de l'extraction des EN contaminées. \texttt{spaCy} propose des modèles de langue du type \textit{large} pour le français\footnote{\texttt{fr\_core\_news\_lg}, \url{https://spacy.io/models/fr}}, le portugais\footnote{\texttt{pt\_core\_news\_lg}, \url{https://spacy.io/models/pt}} et l'anglais\footnote{\texttt{en\_core\_web\_lg}, \url{https://spacy.io/models/en}}. Nous avons favorisé l'usage du modèle \textit{large} (\texttt{spaCy\_lg}) plutôt que du modèle \textit{small} (\texttt{spaCy\_sm}) car la différence principale entre les deux modèles pour les trois langues tient à l'ajout de la vectorisation et des plongements de mots (angl. \textit{embeddings}) dans l'entraînement du modèle \textit{large}\footnote{Modèle anglais : Explosion Vectors (OSCAR 2109 + Wikipedia + OpenSubtitles + WMT News Crawl) (Explosion). Modèles français et portugais :
Explosion fastText Vectors (CBOW, OSCAR Common Crawl + Wikipedia) (Explosion).}.



\subsection{Moins d'hapax : indice de la performance de la REN sur données bruitées ?}
\label{subsec:eval_manu_OCR-IMPACT-NER}
Dans un premier temps, nous proposons une évaluation de l'outil de REN \texttt{spaCy} sur données bruitées, dans laquelle nous comparons les résultats obtenus sur les corpus ELTeC français, anglais et portugais et la TGB, et, leurs transcriptions de ROC\footnote{Dépôt GitHub avec nos données : \url{https://github.com/anonymous}} sans nous appuyer sur un \textit{gold standard}. Nous observons que les fautes d'orthographes provoquées par la ROC ne sont pas systématiquement un frein à la bonne extraction des noms de lieux, comme en témoigne le tableau \ref{tab:typo_erreurs_ocr}.
\begin{table}[h!]
\small
    \centering
   \input{TAB/typologie_erreurs_OCR}
    \caption{Proposition de typologie pour l'évaluation de la REN sur des données issues de la ROC.}
    \label{tab:typo_erreurs_ocr}
\end{table}

En revanche, la concaténation des tokens d'une EN semble être une contamination plus préjudiciable à sa bonne détection. Par ailleurs, l'étude de \cite{DBLP:conf/gis/Koudoro-Parfait21} laisse entendre que (i) le contexte contaminé autour d'une EN pourrait être un facteur de non détection et (ii) un contexte parfaitement propre ne serait pas la garantie que l'EN soit reconnue par le système. Il semble que ces faits soient vérifiables pour les trois langues sur lesquelles nos expériences ont porté. Par ailleurs, certaines entités même très contaminées sont identifiées, comme p.\ ex.\  \textit{``ancehester''} pour \og{}Manchester\fg{}. 


\begin{table}[h!]
    \centering
    \small
    \input{TAB/corpus_bon_moyen_mauvais_OCR}
    \caption{Nombre d'EN identifiées par \texttt{spaCy\_lg} dans les corpus ELTeC  en fonction de différentes qualités de la ROC déterminées par le CER calculé sur le modèle Tess. adapté à la langue du corpus. 
    }
    \label{tab:ELTeC_bon_mauvais}
\end{table}

Le tableau \ref{tab:ELTeC_bon_mauvais} qui répertorie le nombre des types des EN reconnues par \texttt{spaCy} selon la qualité des versions de ROC, illustre le fait que sur les versions de ROC les systèmes de REN récupèrent plus de types d'EN différents, donc que la qualité du texte en entrée influe sur la quantité des types d'EN récupérés en sortie. La qualité des versions de ROC a été évaluée en appliquant les métriques \textit{Character Error Rate} (CER) et \textit{Word Error Rate} (WER) sur les textes de référence et les versions de ROC. On note que plus le WER est élevé, plus la qualité de la transcription baisse, plus le nombre de type d'EN en sortie est élevé. On peut en conclure que (i) le système de REN ramène plus de bruit ou Faux Positif (FP) en sortie quand la ROC est moins bonne et (ii) le nombre des hapax augmente selon que la qualité de la transcription diminue. 
Dans la quantité d'EN surnuméraire détectée sur les versions de ROC par rapport à la référence, figurent des FP mais aussi des formes contaminées des entités, qui sont des hapax, et qui comptent chacune pour un type différent d'EN en plus du type initial de l'EN. Ces phénomènes sont illustrés dans le tableau \ref{tab:FP_VP} qui recense une annotation manuelle des Vrais Positifs (VP) et des FP par type d'EN récupérées par \texttt{spaCy\_lg} sur l'ensemble des EN de référence et celui des versions pour Kraken et Tesseract français. 
\begin{table}[h!]
\small
    \centering
    \input{TAB/DAUDET_FP_VP_2}
    \caption{Annotation manuelle des VP et FP sur les types d'EN reconnus par \texttt{spaCy} pour Daudet\footnote{Compte tenu du temps que prend une annotation manuelle, nous n'avons pas annoté tous les corpus et nous ne disposons pas d'un \textit{gold standard} global.}.}
    \label{tab:FP_VP}
\end{table}
On retrouve bien (i) plus de FP et (ii) plus de VP qui sont des hapax sur les transcriptions OCR que sur la référence. Il y a donc plus de types différents d'entités sur la sortie de la ROC que sur la sortie de la Réf., car, comme l'illustre le tableau \ref{tab:EN_contamines_Variantes}, les variantes d'une entités peuvent être nombreuses. 



\begin{table}[h!]
\small
    \centering
    \input{TAB/EN_contamines_Variantes}
    \caption{REN sur des formes contaminées de l'EN ``Ferme des Ormeaux'', {\normalfont La petite Jeanne}, Carraud.}
    \label{tab:EN_contamines_Variantes}
\end{table}

L'analyse du tableau \ref{tab:ELTeC_bon_mauvais} montre qu'il existe de problèmes relatifs à la déperdition de données lors de la transcription de ROC. Des EN n'ont pas été transcrites par l'outil de ROC et donc apparaissent comme du silence. Néanmoins, il ne s'agit pas d'un FN de l'outil de REN, mais d'un FN de l'outil de ROC. Le cas Reynolds, pour lequel seuls 111 types d'EN ont été récupérés dans la configuration Kraken-\texttt{spaCy\_lg}, en est l'exemple, plus de 90\% des pages n'ont pas été transcrites à cause du flou sur les pages concernées. D'autres textes ont connu le même sort dans de très moindre proportions. Nous n'avons pas mesuré l'impact de cette non transcription car elle était en faible proportion sur tout le corpus.

%Enfin, il peut arriver, plus rarement, que des entités ne soient pas détectées sur la version de Réf. mais le soient sur la version de ROC. Il ne s'agit pas véritablement de FP, mais d'une erreur du système même en contexte non bruité. 

Enfin, il peut arriver, plus rarement, que des entités ne soient pas détectées sur la version de référence (réf.). mais le soient sur la version de ROC. Il peut s'agir d'une part du fait que la version de ROC contient du texte en plus de la version de réf., par exemple les notes de bas de pages transcrites par la ROC mais non prises en compte par les auteurs de la référence, ou, d'autre part  d’une erreur du système même en contexte non bruité. Dans ces deux cas, il ne s’agit pas véritablement de FP, et ces cas particuliers viennent poser les limites de l'évaluation de la tâche de REN sur données bruitées.


%%%________ Plus d'EN sur texte bruité
\subsection{Usage des intersections : une évaluation trop stricte ?}
\label{subsec:inter_OCR-IMPACT-NER}
Pour automatiser nos analyses et pouvoir les conduire à plus grande échelle nous avons décidé de calculer et représenter les intersections entre les ensembles des EN reconnues sur la version de réf. et celles obtenues sur les versions de ROC. Les versions de ROC et les textes de référence ont été annotés automatiquement avec \texttt{spaCy\_lg}. Nous nous servons de ces derniers comme vérité terrain \footnote{Jeu de données de REN constitué pour l'évaluation.}. Nous avons calculé les intersections pour chacun des corpus (ELTeC français, anglais et portugais, et TGB) de manière globale. 
Pour ce faire nous avons fait correspondre les entités de chacun des textes de référence avec celles de leurs versions de ROC. Ainsi, dans le cas du corpus ELTeC français l'EN ``Paris'' repérée dans le texte de référence pour Daudet, n'est pas la même que l'EN ``Paris'' récupérée dans le texte de référence de Noailles. Il en va de même pour les différentes autres configurations (où, sous le terme, \og{}configuration\fg{} on désigne la combinaison d'un modèle de ROC et d'un modèle de REN, p. ex. les résultats de la configuration Kraken-\texttt{spaCy\_lg}). \textit{cf.} le tableau \ref{tab:config}).

La figure \ref{fig:intersection_globale-kraken-tess} rend compte de cette évaluation stricte dans laquelle chaque token de l'ensemble de Réf. est comparé avec chaque token de l'ensemble de ROC. Pour être considéré comme un VP une EN de l'ensemble de ROC doit être orthographiée de manière identique à une EN de l'ensemble de Réf. Dans ce graphique, le cercle de gauche - ``EN Réf'' - qui comprend les EN de la Réf. et l'intersection entre les deux cercles sont considérés comme représentant les VP. Le cercle de droite - ``EN\_nom-de-ROC'' - qui représente les EN récupérées uniquement sur la version de ROC est considéré comme comportant les FP de la sortie de REN. Cependant, lorsque nous avons observé les résultats il s'est avéré que (i) les EN issues de la ROC portant des contaminations et n'étant pas strictement orthographiées comme leur pendant de référence et (ii) les EN qui sont bien récupérées sur la ROC et pas sur la Réf. même rares sont considérées par la machine comme des FP. Cet état de fait créer donc un biais dans nos évaluations. Entre autres exemples l'EN \textit{Ormeaux} n'est pas strictement identique à sa version contaminée \textit{Ormaeuux} ou ``\textit{Dconshire}'' pour ``\textit{Devonshire}''. Ces EN contaminées sont comptées comme des FP et ajoutées à la liste des hapax, ce qui vient gonfler artificiellement le nombre des FP dans l'ensemble des EN de la ROC. Il s'agit en fait de Faux Faux Positif (FFP) autrement dit de VP masqués par la rectitude de l'alignement inhérent au mode d'évaluation adopté. Ces différents cas sont décrits en détail dans la sous-section \ref{subsec:Typologie_OCR-IMPACT-NER}.
 Enfin, dans l'ensemble des EN reconnues sur les versions de ROC nous remarquons que la majorité des EN présentes dans les résultats obtenus sur la ROC sont effectivement présentes dans les résultats obtenus sur les versions de référence, comme le présente le tableau \ref{tab:FP_VP}. Il n'y a donc pas de véritable déperdition des VP. 


%%%______________ Plus d'EN mais est-ce que c'est du bruit ?








\begin{figure}[h!]
    \begin{minipage}{7cm}
  \begin{subfigure}{1\textwidth}
  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Kraken_spacy-lg-concat_intersection.png} 
  \caption{Kraken --\texttt{spaCy-lg}}
  \label{fig:ELTeCFRA_Kraken_spacy-lg-concat_intersection}
  \end{subfigure}
  \end{minipage}
  \begin{minipage}{7cm}
  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Kraken_stanza-concat_intersection.png} %%%% ancienne figure 07/02/2024
  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Tess. fr_spacy-lg-concat_intersection.png}
  \caption{Tess. fr. -- \texttt{spaCy\_lg}}
 % \label{fig:ELTeCFRA_Kraken_stanza-concat_intersection}
  \end{subfigure}
    \end{minipage}
%\begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Kraken -- jspl-ELTeCfr_spacy-lg-concat_intersection.png} 
%  \caption{Kraken corrigé ELTeC-fr -- \texttt{spaCy-lg}}
%  \label{fig:ELTeCFRA_Kraken -- jspl-ELTeCfr_spacy-lg-concat_intersection}
%  \end{subfigure}
%  \end{minipage}
%  \begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Kraken -- jspl-ELTeCfr_stanza-concat_intersection.png}
%  \caption{Kraken corrigé ELTeC-fr -- \texttt{stanza}}
%  \label{fig:ELTeCFRA_Kraken -- jspl-ELTeCFR_stanza-concat_intersection}
%  \end{subfigure}
%    \end{minipage}
\caption{Intersections pour les configurations Kraken-\texttt{spaCy\_lg} et Tess. fr.-\texttt{spaCy\_lg}, pour le corpus ELTeC français.}
%\label{fig:intersection_globale-kraken}
\label{fig:intersection_globale-kraken-tess}
\end{figure}

%%%%%%%%%%%% ancienne sous-figure 07/02/2024
%\begin{figure}[h!]
%    \begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Tess. fr_spacy-lg-concat_intersection.png} 
%  \caption{Tess. fr. --\texttt{spaCy\_lg}}
%  \label{fig:ELTeCFRA_Tess. fr_spacy-lg-concat_intersection}
%  \end{subfigure}
%  \end{minipage}
%  \begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Tess. fr_stanza-concat_intersection.png}
%  \caption{Tess. fr. -- \texttt{stanza}}
% % \label{fig:ELTeCFRA_Tess. fr_stanza-concat_intersection}
%  \end{subfigure}
%    \end{minipage}

%\begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Tess. fr -- jspl-ELTeCfr_spacy-lg-concat_intersection.png} 
%  \caption{Tess. fr. corrigé -- \texttt{spaCy\_lg}}
%  \label{fig:ELTeCFRA_Tess. fr -- jspl-ELTeCfr_spacy-lg-concat_intersection}
%  \end{subfigure}
% \end{minipage}
%  \begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Tess. fr -- jspl-ELTeCfr_stanza-concat_intersection.png}
%  \caption{Tess. fr. corrigé -- \texttt{stanza}}
%  \label{fig:ELTeCFRA_Tess. fr -- jspl-ELTeCfr_stanza-concat_intersection}
%  \end{subfigure}
%    \end{minipage}
%\begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Tess. fr -- jspl-fr_spacy-lg-concat_intersection.png} 
%  \caption{Tess. fr. corrigé -- \texttt{spaCy\_lg}}
%  \label{fig:ELTeCFRA_Tess. fr -- jspl-fr_spacy-lg-concat_intersection}
%  \end{subfigure}
%  \end{minipage}
%  \begin{minipage}{7cm}
%  \begin{subfigure}{1\textwidth}
%  \includegraphics[width=1\textwidth]{IMAGES/INTERSECTIONS_GLOBALES/ELTeCFRA_Tess. fr -- jspl-fr_stanza-concat_intersection.png}
%  \caption{Tess. fr. corrigé -- \texttt{stanza}}
%  \label{fig:ELTeCFRA_Tess. fr -- jspl-fr_stanza-concat_intersection}
%  \end{subfigure}
%    \end{minipage}
%\caption{Intersections pour la configuration Tess-\texttt{spaCy\_lg} corrigées avec le modèle pré-entrainer de JamSpell et le modèle ELTeC, pour le corpus ELTeC français.}
%\label{fig:intersection-globale-tess}
%\end{figure}



\subsection{Typologie des contaminations de la ROC pour une évaluation fine}
\label{subsec:Typologie_OCR-IMPACT-NER}
En regard des différentes observations que nous venons d'apporter et parce que nous souhaitons rendre compte de la complexité de ces cas réels, tels que nous les exposons dans les parties \ref{subsec:eval_manu_OCR-IMPACT-NER} et \ref{subsec:inter_OCR-IMPACT-NER} ainsi que dans le tableau \ref{tab:typo_eval}, nous proposons d'établir une typologie pour l'évaluation des contaminations de la ROC sur la REN, élargissant la classification standard des vrais/faux positifs/négatifs. Si les FP sont qualifiés de bruit et les FN de silence, nous avons repéré que l'évaluation par calcul des intersections cache des phénomènes de surestimation ou sous-estimation du bruit et du silence dans les données. Cette typologie permettrait d'établir quels sont les vrais bruits, autrement dit les vrais FP et les vrais silences les vrais VN. 


\begin{itemize}
\footnotesize

	\item[] \textsc{Cas attendus}
	
	
	\begin{description}

		\item[Vrais positifs (VP)]: EN détectées dans les deux versions.
		\item[Vrais négatifs (VN)]: Aucune EN à reconnaître dans les deux versions.
		\item[Faux positif (FP)]: EN détectées à tort dans la version de ROC (bruit de la REN).
		\item[Faux négatif (FN)]: EN manquantes dans la version de ROC (silence de la REN).
	\end{description}
	
		
	\item[] \textsc{Sous évaluation du bruit et du silence de la REN}
	
	
	\begin{description}
	
		\item[Faux vrais positifs (FVP)]: EN détectées à tort dans les deux versions.
	
	 	\item[Faux vrais négatifs (FVN)] : EN manquantes dans les deux versions.
	 \end{description}
	 
	 \item[] \textsc{sur évaluation du bruit et du silence de la REN}
	
	\begin{description}
	
		\item[Faux faux positifs (FFP)]: EN détectées dans les versions de ROC mais pas dans le texte de référence (EN manquantes dans la référence$^{(i)}$ ou EN contaminées détectées dans la version de ROC$^{(ii)}$).
		
 		\item[Faux faux négatifs (FFN)]: EN  détectées à tort dans le texte de référence.

	\end{description}

\end{itemize}

\begin{table}[h!]
    \centering
    \input{TAB/typologie_erreurs_OCR_exemple.tex}

    \caption{Exemples de cas réels d'EN justifiant de la typologie d'évaluation de l'impact des erreurs de la ROC sur la REN.}
    \label{tab:typo_eval}
\end{table}

%%%% Matrice mise de côté pour le moment
%En regard des différentes sorties de REN obtenues avec \texttt{spaCy} et \texttt{stanza}, nous proposons une typologie des contaminations de ROC, en élargissant la classification standard des vrais/faux positifs/négatifs. Les définitions correspondant à chaque catégorie (indiquées dans la matrice de confusion, tableau \ref{tab:Matrice_erreur_OCR_V2}) sont les suivantes :
%\begin{table}[H]
 %   \centering
  %  \input{TAB/type_erreurs_OCR_V1}
   % \caption{Typologie des contaminations d'OCR étendue.}
    %\label{tab:my_label}
%\end{table}



%\begin{table}[h!]
%    \centering
%    \input{TAB/Matrice_erreur_OCR_V2}
%
%    \caption{Matrice de confusion pour l'évaluation de la REN sur des sorties ROC bruitées. IN = correctement détecté ; \sout{IN} = incorrectement détecté ; OUT = correctement ignoré ; \sout{OUT} = oublié ; case noire = cas impossible}
%    \label{tab:Matrice_erreur_OCR_V2}
%\end{table}

%\begin{table}[]
 %   \centering
  %  \input{TAB/Matrice_erreur_OCR_V1}

   % \caption{Matrice de confusion pour l'évaluation de la REN sur des sorties ROC bruitées. IN = correctement détecté ; \sout{IN} = incorrectement détecté ; OUT = correctement ignoré ; \sout{OUT} = oublié}
    %\label{tab:Matrice_erreur_OCR_V1}
%\end{table}

\subsection{\'Evaluation supervisée des contaminations de la ROC sur un corpus annoté}
\label{subsec:eval_supervis_OCR-IMPACT-NER}
Afin d'évaluer de manière supervisée l'influence du bruit de la ROC sur la REN, nous avons annoté un échantillon du corpus ELTeC français.
 Nous avons choisi de nous limiter aux quatre catégories présentes dans \texttt{spaCy} (Lieux, Personnes, Organisations et Divers).
  Nous avons tout d'abord annoté un échantillon de 3~000 tokens d'une œuvre  puis réalisé une adjudication pour régler les désaccords. 
  Nous avons ensuite annoté 5 000 tokens de 3 versions (Référence, Tesseract et Kraken) de deux œuvres (Daudet et Maupassant). %L'accord inter-annotateur (Kappa de  Fleiss \cite{fleiss2013statistical}) était de $0.877$, significativement plus élevé sur la version de référence ($0.905$) que sur les versions ROC.
L'accord inter-annotateur (Kappa de  Fleiss \cite{fleiss2013statistical}) était de $0.905$ sur la version de réf., ce qui est, significativement plus élevé que le score obtenu sur les versions de ROC : $0.877$.
 Nous avons pu observer que les désaccords étaient plus nombreux sur l'annotation des versions de ROC du fait des problèmes de tokenisation.

Grâce à un système de vote majoritaire, nous avons fusionné les annotations pour obtenir un \textit{gold standard} sur chaque version de chaque œuvre.
 Nous avons évalué \texttt{spaCy\_lg} sur cet échantillon, dont les résultats sont présentés dans le tableau \ref{tab:eval-supervise}. À cet effet, nous proposons deux types d'évaluation que nous appelons ici \og{}stricte\fg{} et \og{}souple\fg{}. La première configuration ne prend en compte que les correspondances exactes des EN (p. ex. le mot contaminé \textit{Acques} sera considéré comme un FP, même s'il renvoie au VP \textit{Jacques}). En revanche, la configuration \og{}souple\fg{} considère une EN comme correcte, quelle que soit sa taille (une partie ou l'intégralité de l'EN, auquel cas la forme contaminée \textit{Acques} sera considéré comme un VP. Par rapport à l'évaluation dite \og{}globale\fg{} (qui concerne tous les types d'EN), nous pouvons remarquer d'une part que les résultats obtenus sur les versions Tesseract sont meilleurs que ceux obtenus sur les versions Kraken dans les configurations stricte et souple.
  En outre, ce qui est plus étonnant, dans le cas de l'évaluation souple, ces résultats sont même meilleurs que ceux obtenus sur la version de réf..
  D'autre part, nous remarquons que là aussi la faiblesse apparente des résultats de la REN obtenus sur des versions de ROC est principalement due à des problèmes d'alignement entre les tokens contaminés et les tokens de référence. 
%  comment expliquez-vous la meilleure performance sur les entités de lieux dans la configuration stricte ? [càd. pourquoi la F-mesure est meilleure sur les lieux que sur les autres EN avec l'évaluation stricte dans le tableau 8 ?]
  Enfin, le fait que le F-score soit meilleur sur les EN de lieux que sur le globale dans la configuration stricte peut s'expliquer par le fait que les EN de lieux comptent le plus souvent un token dans nos corpus (p. ex. \textit{Paris}), au contraire des EN de personnes. Par exemple, \textit{Daniel Eyssette}, qui est écrit \textit{Daniel Ey-sset-te}\footnote{Les tirets représentent des retours à la ligne.}. Ainsi, la deuxième partie de l'EN est divisée en trois éléments distincts, ce qui a vraisemblablement posé problème à l'évaluation globale et diminué les F-scores. Dès lors, le rappel sur les noms de personnes descend beaucoup plus ; nous obtenons moins de VP, et les FP augmentent moins sur les lieux.
\begin{table}[h!]
\centering
\scriptsize{\begin{tabular}{l|ccc|ccc}
 %Versions
 &		&	GLOBALE	&		&		&	LIEUX	&		\\
\hline
 \hline
\textbf{Souple}	&	Rappel		&Précision	&	F-sc.	&	Rappel&		Précision		&F-sc.	\\
 \hline
Kraken	&	49.57	&	73.72 	&	59.28	&	48.84	&	52.50	&	50.60	\\
													
Tesseract	&	51.53	&	77.63	&	61.94	&	56.41	&	57.89	&	57.14	\\
													
Référence	&	49.78	&	77.55	&	60.64	&	53.49	&	53.49	&	53.49	\\
\hline
\hline
\textbf{Stricte}	&	Rappel		&Précision	&	F-sc.	&	Rappel&		Précision		&F-sc.	 \\
\hline
Kraken	&	18.26	&	58.82	&	27.87	&	43.24	&	45.71	&	44.44	\\
Tesseract	&	21.00	&	68.66	&	32.16	&	43.33	&	44.83	&	44.07	\\
Référence	&	21.62	&	69.57	&	32.98	&	41.18	&	45.16	&	43.08	\\
\hline
\hline
\end{tabular}}
\caption{Evaluation de \texttt{spaCy\_lg} sur un échantillon annoté de 10~000 tokens dans trois versions textuelles différentes\label{tab:eval-supervise} en configuration souple et en configuration stricte (GLOBALE : tous les types d'entités, LIEUX: seulement les lieux).}
 \end{table}

Nous concluons de ces travaux préliminaires d'évaluation de l'impact du bruit de la ROC sur la tâche de REN que les erreurs de la ROC ne sont pas toujours un frein à la bonne conduite de la tâche de REN, et que la présence de nombreux hapax dans une sortie de REN peut être le signe qu'il existe des formes contaminées d'EN. Néanmoins, nous constatons qu'il est difficile d'évaluer de manière stricte le silence et le bruit réel dans les sorties de REN sur données bruitées par la ROC, puisque l'alignement entre les versions de référence et de ROC du fait des formes contaminées des EN est une tâche ardue. Jusque là nous n'avons pas pu trouver de manière convaincante de calculer un F-score et nous commençons à entrevoir les limites de l'usage des intersections.
%\textbf{Dans la suite de cet article nous traitons de l'impact des corrections des transcription ROC sur la REN et nous envisageons l'usage de l'indice de Jaccard, ou d'une similarité cosinus, mais aussi de NERVAL, outil qui calcule le F-score sur des sorties de REN à partir des données bruités. }


